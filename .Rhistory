data <- data[c("ProbeID", "SID", "Date_Time","Temp","UOM","Collector")]
#return TRUE is all rows were entered, Error otherwise, ALL OR NOTHING
dbWriteTable(db,'probe_temps', data, append=T); #all or nothing append!!!
#insert data into the DB=====================================
count=count+1
}
#If Error - Identify File####
files[count+1]#File with Error
files[1:count]#Files that were successfully uploaded
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.afupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.afupld.cnt <-dim(rows.afupld)
rows.afupld.cnt[1]
dbDisconnect(db);
db_path <- 'S:/M_Kozlak/Temperature/TemperatureDB/'
db <- dbConnect(SQLite(), dbname=paste(db_path,"stream.temperature.sqlite",sep=''));
#Identify and Cnt the Rows Before Upload##############
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.bfupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.bfupld.cnt <-dim(rows.bfupld)
rows.bfupld.cnt[1]
csv_dir <- 'S:/M_Kozlak/Temperature/TemperatureDB/DataForUpload/Uploaded/FilesForDB_2016_Fall/'
files <- list.files(csv_dir,'*.csv'); #only csv files extensions
m <- length(files);
count=0
for(j in 1:m){ #for each csv files in the directory
data <- read.table(paste(csv_dir,files[j],sep=''),sep=',',header=T, stringsAsFactors=F,
na.strings=c("","NA"));
data <- data[!is.na(data[,'SID']),]
names(data)<- c("Date_Time","Temp","UOM","ProbeID","SID","Collector")
n <- dim(data)[1] # number of rows in the csv file
for (i in 1:n){
data[i,'Date_Time'] <- csv_to_db_datetime(data[i,'Date_Time']);
}
#reorder by column name for insert to match the SQLite DB
data <- data[c("ProbeID", "SID", "Date_Time","Temp","UOM","Collector")]
#return TRUE is all rows were entered, Error otherwise, ALL OR NOTHING
dbWriteTable(db,'probe_temps', data, append=T); #all or nothing append!!!
#insert data into the DB=====================================
count=count+1
}
#If Error - Identify File####
files[count+1]#File with Error
files[1:count]#Files that were successfully uploaded
SQL<- "SELECT ProbeID, SID,min(Date_Time) as Date_Time,max(Date_Time) as Date_Time
FROM probe_temps
GROUP BY ProbeID,SID;"
rows.afupld <- dbGetQuery(conn=db,SQL); #returns data.frame
rows.afupld.cnt <-dim(rows.afupld)
rows.afupld.cnt[1]
dbDisconnect(db);
library(RSQLite)
library(plyr)
library(ggplot2)
db_path <- 'S:/M_Kozlak/Temperature/TemperatureDB/' #on windows like this
db <- dbConnect(SQLite(), dbname=paste(db_path,"stream.temperature.sqlite",sep=''));
names  <- dbListTables(db);                        # The tables in the database
fields <- dbListFields(db, "probe_temps");    # The columns in a table
table  <- dbReadTable(db, "probe_temps");  # get the whole table as a data.frame
table$day <- substr(table$Date_Time,6,10)##Add column of data that includes month_day
table$month<- substr(table$Date_Time,6,7)##Add column of data that includes month
table$year<- substr(table$Date_Time,1,4)##Add column of data that includes year
AvgDay <- ddply(table,c("ProbeID","SID","day","month","year","Collector","UOM"),summarize,mean=mean(Temp),min=min(Temp),
max=max(Temp),maxmin= (max(Temp)-min(Temp)),N=length(Temp))#AvgByDay
AvgDay$Flag [AvgDay$N<24|AvgDay$min<0|AvgDay$maxmin>5]<-1
write.csv(AvgDay,"S:/M_Kozlak/Temperature/TemperatureDB/Summary_DB/TempAvgDay.csv",row.names=FALSE)
windows(title="Daily Temp Flux By Month",5,5)
ggplot(AvgDay,aes(x=month,y=maxmin,fill=month))+
geom_boxplot()+
ylim(NA,10)
AvgDayMonth<-subset(AvgDay,month=="10")
quantile(AvgDayMonth$maxmin,c(.25,.50,.75,.90,.99))
yr <- 2015 #Specify the year for metric calcs
temp <- subset(table,year==yr)##Subset by Year
AvgDay <- ddply(temp,c("SID","day","month","year"),summarize,mean=mean(Temp),N=length(Temp))#AvgByDay
SummerMonths <- subset(AvgDay,month %in% c('06','07','08')& N==24)#Subset Summer Months
AvgSummerTemp <- ddply(SummerMonths,"SID",summarize,SummerTemp=mean(mean),SN=length(mean))#Summer Temp Month
AvgSummerTemp$TempCatS <- AvgSummerTemp$SummerTemp
AvgSummerTemp$TempCatS<- as.character(AvgSummerTemp$TempCat)
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp<18.29]<-"Cold"
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp>21.7]<-"Warm"
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp>=18.29&AvgSummerTemp$SummerTemp<=21.7]<-"Cool"
July <- subset(AvgDay,month %in% '07'& N==24)#Subset
AvgJulyTemp <- ddply(July,"SID",summarize,JulyTemp=mean(mean),JN=length(mean))#Summer Temp Month
AvgJulyTemp$TempCatJ <- AvgJulyTemp$JulyTemp
AvgJulyTemp$TempCatJ<- as.character(AvgJulyTemp$TempCat)
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp<18.45]<-"Cold"
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp>22.30]<-"Warm"
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp>=18.45&AvgJulyTemp$JulyTemp<=22.30]<-"Cool"
MaxDailyTemp <- ddply(AvgDay,"SID",summarize,MaxD = max(mean),MN=length(mean))
MaxDailyTemp$TempCatM <- MaxDailyTemp$MaxD
MaxDailyTemp$TempCatM <- as.character(MaxDailyTemp$MaxD)
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD<22.4]<- "Cold"
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD>26.3]<- "Warm"
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD>=22.4&MaxDailyTemp$MaxD<=26.3]<- "Cool"
TempMetrics <- merge(AvgSummerTemp,AvgJulyTemp,by="SID")
TempMetrics <-merge(TempMetrics,MaxDailyTemp,by="SID")
TempMetrics$Flag [TempMetrics$SN<92|TempMetrics$JN <31] <- 1
write.csv(TempMetrics,paste("S:/M_Kozlak/Temperature/TemperatureDB/MetricCalcs/TempMetrics",yr,".csv"),row.names=FALSE)
yr <- 2016 #Specify the year for metric calcs
temp <- subset(table,year==yr)##Subset by Year
AvgDay <- ddply(temp,c("SID","day","month","year"),summarize,mean=mean(Temp),N=length(Temp))#AvgByDay
##Avg Summer Temp##
SummerMonths <- subset(AvgDay,month %in% c('06','07','08')& N==24)#Subset Summer Months
AvgSummerTemp <- ddply(SummerMonths,"SID",summarize,SummerTemp=mean(mean),SN=length(mean))#Summer Temp Month
AvgSummerTemp$TempCatS <- AvgSummerTemp$SummerTemp
AvgSummerTemp$TempCatS<- as.character(AvgSummerTemp$TempCat)
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp<18.29]<-"Cold"
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp>21.7]<-"Warm"
AvgSummerTemp$TempCatS[AvgSummerTemp$SummerTemp>=18.29&AvgSummerTemp$SummerTemp<=21.7]<-"Cool"
##Avg July Temp##
July <- subset(AvgDay,month %in% '07'& N==24)#Subset
AvgJulyTemp <- ddply(July,"SID",summarize,JulyTemp=mean(mean),JN=length(mean))#Summer Temp Month
AvgJulyTemp$TempCatJ <- AvgJulyTemp$JulyTemp
AvgJulyTemp$TempCatJ<- as.character(AvgJulyTemp$TempCat)
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp<18.45]<-"Cold"
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp>22.30]<-"Warm"
AvgJulyTemp$TempCatJ[AvgJulyTemp$JulyTemp>=18.45&AvgJulyTemp$JulyTemp<=22.30]<-"Cool"
##Max Daily Mean##
MaxDailyTemp <- ddply(AvgDay,"SID",summarize,MaxD = max(mean),MN=length(mean))
MaxDailyTemp$TempCatM <- MaxDailyTemp$MaxD
MaxDailyTemp$TempCatM <- as.character(MaxDailyTemp$MaxD)
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD<22.4]<- "Cold"
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD>26.3]<- "Warm"
MaxDailyTemp$TempCatM [MaxDailyTemp$MaxD>=22.4&MaxDailyTemp$MaxD<=26.3]<- "Cool"
##Combine and Export Metrics By Year
TempMetrics <- merge(AvgSummerTemp,AvgJulyTemp,by="SID")
TempMetrics <-merge(TempMetrics,MaxDailyTemp,by="SID")
TempMetrics$Flag [TempMetrics$SN<92|TempMetrics$JN <31] <- 1
write.csv(TempMetrics,paste("S:/M_Kozlak/Temperature/TemperatureDB/MetricCalcs/TempMetrics",yr,".csv"),row.names=FALSE)
dbDisconnect(db);
setwd('P:/Projects/GitHub_Prj/DO_Data')
######assume one header line and tab delimited structure, with # as a comment out to skip#####
parse_fstat<-function(fstat_lines,skip='#',delim='\t'){
x<-1;
while(x<length(fstat_lines) && startsWith(fstat_lines[x],skip)){
x<-x+1;
}
header<-strsplit(fstat_lines[x],delim)[[1]];
D<-as.data.frame(matrix('',ncol=length(header),nrow=length(fstat_lines)-x),stringsAsFactors=F);
colnames(D)<-header;
for(i in x+2:length(fstat_lines)){
r<-strsplit(fstat_lines[i],delim)[[1]];
D[i-x-1,1:length(r)]<-r;
}
D
}
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01124000'
start_date  <-'2015-06-15';
end_date    <-'2015-09-22';
parameterCd <-'00060,00065,00010,00095,00300,00301';
###Combine into url for site#########
DO_parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(DO_parts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
DO[1:10,]
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01195510'
start_date  <-'2015-06-15';
end_date    <-'2015-09-22';
parameterCd <-'00060,00065,00010,00095,00300,00301';
###Combine into url for site#########
DO_parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(DO_parts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
DO[1:10,]
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01196528'
start_date  <-'2015-06-15';
end_date    <-'2015-09-22';
parameterCd <-'00060,00065,00010,00095,00300,00301';
###Combine into url for site#########
DO_parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(DO_parts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
DO[1:10,]
str(DO)
colnames(DO)
length(colnames(DO))
colnames(DO)[5:length(colnames(DO)),]
colnames(DO)[5:length(colnames(DO))]
length(colnames(DO)[5])
nchar(colnames(DO)[5])
colnames(DO)[5]<-substr(colnames(DO)[5],6,nchar(colnames(DO)[5]))
colnames(DO)
DO<-parse_fstat(readLines(DO_url))
for i in [5:length(colnames(DO))]{
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
}
for (i in [5:length(colnames(DO)))]{
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
}
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
}
colnames(DO)
DO<-parse_fstat(readLines(DO_url))
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
ifelse(colnames(DO)[i]=="00010",colnames(DO)[i]<-"temp.water",colnames(DO)[i])
}
colnames(DO)
DO<-parse_fstat(readLines(DO_url))
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
ifelse(colnames(DO)[i]=="00010",colnames(DO)[i]<-"temp.water",
ifelse(colnames(DO)[i]=="00095",colnames(DO)[i]<-"cond",
ifelse(colnames(DO)[i]=="00300",colnames(DO)[i]<-"DO.obs",
ifelse(colnames(DO)[i]=="00301",colnames(DO)[i]<-"DO.sat",
ifelse(colnames(DO)[i]=="00065",colnames(DO)[i]<-"depth",
ifelse(colnames(DO)[i]=="00060",colnames(DO)[i]<-"discharge"),
colnames(DO)[i]<-colnames(DO)[i])))))
}
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
ifelse(colnames(DO)[i]=="00010",colnames(DO)[i]<-"temp.water",
ifelse(colnames(DO)[i]=="00095",colnames(DO)[i]<-"cond",
ifelse(colnames(DO)[i]=="00300",colnames(DO)[i]<-"DO.obs",
ifelse(colnames(DO)[i]=="00301",colnames(DO)[i]<-"DO.sat",
ifelse(colnames(DO)[i]=="00065",colnames(DO)[i]<-"depth",
ifelse(colnames(DO)[i]=="00060",colnames(DO)[i]<-"discharge",
colnames(DO)[i]<-colnames(DO)[i])))))
}
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
ifelse(colnames(DO)[i]=="00010",colnames(DO)[i]<-"temp.water",
ifelse(colnames(DO)[i]=="00095",colnames(DO)[i]<-"cond",
ifelse(colnames(DO)[i]=="00300",colnames(DO)[i]<-"DO.obs",
ifelse(colnames(DO)[i]=="00301",colnames(DO)[i]<-"DO.sat",
ifelse(colnames(DO)[i]=="00065",colnames(DO)[i]<-"depth",
ifelse(colnames(DO)[i]=="00060",colnames(DO)[i]<-"discharge",
colnames(DO)[i]<-colnames(DO)[i]))))))
}
colnames(DO)
DO<-parse_fstat(readLines(DO_url))
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
ifelse(colnames(DO)[i]=="00010",colnames(DO)[i]<-"temp.water",
ifelse(colnames(DO)[i]=="00095",colnames(DO)[i]<-"cond",
ifelse(colnames(DO)[i]=="00300",colnames(DO)[i]<-"DO.obs",
ifelse(colnames(DO)[i]=="00301",colnames(DO)[i]<-"DO.sat",
ifelse(colnames(DO)[i]=="00065",colnames(DO)[i]<-"depth",
ifelse(colnames(DO)[i]=="00060",colnames(DO)[i]<-"discharge",
colnames(DO)[i]<-colnames(DO)[i]))))))
}
colnames(DO)
DO$datetimePOS<- as.POSIXct(DO$datetime,format="%Y-%m-%d %H:%M:%S", tz='America/New_York')
DO[1:10,]
DO$datetimePOS<- as.POSIXct(DO$datetime,format="%Y-%m-%d %H:%M", tz='America/New_York')
DO[1:10,]
lubridate::tz(DO$datetimePOS[1])
install.packages("StreamMetabolism")
install.packages("streamMetabolizer", dependencies=TRUE,
repos=c("https://owi.usgs.gov/R","https://cran.rstudio.com"))
library(streamMetabolizer)
DO$solar.time<- calc_solar_time(DO$datetimePOS,longitude=-106.3)
DO[1:10,]
lubridate::tz(DO$solar.time)
DO_url
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01124000'
start_date  <-'2015-06-15';
end_date    <-'2015-09-22';
parameterCd <-'00060,00065,00010,00095,00300,00301';
###Combine into url for site#########
DO_parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(DO_parts,sep='',collapse='');
DO_url
DO<-parse_fstat(readLines(DO_url))
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
ifelse(colnames(DO)[i]=="00010",colnames(DO)[i]<-"temp.water",
ifelse(colnames(DO)[i]=="00095",colnames(DO)[i]<-"cond",
ifelse(colnames(DO)[i]=="00300",colnames(DO)[i]<-"DO.obs",
ifelse(colnames(DO)[i]=="00301",colnames(DO)[i]<-"DO.sat",
ifelse(colnames(DO)[i]=="00065",colnames(DO)[i]<-"depth",
ifelse(colnames(DO)[i]=="00060",colnames(DO)[i]<-"discharge",
colnames(DO)[i]<-colnames(DO)[i]))))))
}
DO$datetimePOS<- as.POSIXct(DO$datetime,format="%Y-%m-%d %H:%M", tz='America/New_York')
DO$solar.time<- calc_solar_time(DO$datetimePOS,longitude=-106.3)
colnames(DO)
DO<- c(DO$site_no,DO$solar.time,DO$temp.water,DO$discharge,DO$depth,DO$cond,DO$DO.obs,DO$DO.sat)
dim(DO)
DO[1:10,]
DO_url<-paste(DO_parts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
ifelse(colnames(DO)[i]=="00010",colnames(DO)[i]<-"temp.water",
ifelse(colnames(DO)[i]=="00095",colnames(DO)[i]<-"cond",
ifelse(colnames(DO)[i]=="00300",colnames(DO)[i]<-"DO.obs",
ifelse(colnames(DO)[i]=="00301",colnames(DO)[i]<-"DO.sat",
ifelse(colnames(DO)[i]=="00065",colnames(DO)[i]<-"depth",
ifelse(colnames(DO)[i]=="00060",colnames(DO)[i]<-"discharge",
colnames(DO)[i]<-colnames(DO)[i]))))))
}
DO$datetimePOS<- as.POSIXct(DO$datetime,format="%Y-%m-%d %H:%M", tz='America/New_York')
DO$solar.time<- calc_solar_time(DO$datetimePOS,longitude=-106.3)
DO<- DO[,c(DO$site_no,DO$solar.time,DO$temp.water,DO$discharge,DO$depth,DO$cond,DO$DO.obs,DO$DO.sat)]
DO[1:10,]
DO<- DO[,c("site_no","solar.time","temp.water","discharge","depth","cond","DO.obs","DO.sat")]
str(DO)
colnames(DO)
for (i in 3:length(colnames(DO))){
colnames(DO)[i]=as.numeric(colnames(DO)[i])
}
str(DO)
DO<-parse_fstat(readLines(DO_url))
for (i in 5:length(colnames(DO))){
colnames(DO)[i]<-substr(colnames(DO)[i],7,nchar(colnames(DO)[i]))
ifelse(colnames(DO)[i]=="00010",colnames(DO)[i]<-"temp.water",
ifelse(colnames(DO)[i]=="00095",colnames(DO)[i]<-"cond",
ifelse(colnames(DO)[i]=="00300",colnames(DO)[i]<-"DO.obs",
ifelse(colnames(DO)[i]=="00301",colnames(DO)[i]<-"DO.sat",
ifelse(colnames(DO)[i]=="00065",colnames(DO)[i]<-"depth",
ifelse(colnames(DO)[i]=="00060",colnames(DO)[i]<-"discharge",
colnames(DO)[i]<-colnames(DO)[i]))))))
}
DO$datetimePOS<- as.POSIXct(DO$datetime,format="%Y-%m-%d %H:%M", tz='America/New_York')
DO$solar.time<- calc_solar_time(DO$datetimePOS,longitude=-106.3)
DO<- DO[,c("site_no","solar.time","temp.water","discharge","depth","cond","DO.obs","DO.sat")]
for (i in 3:length(colnames(DO))){
as.numeric(colnames(DO)[i])
}
str(DO)
for (i in 3:length(colnames(DO))){
transform(DO,DO[,i]=as.numeric(DO[,i]))
}
DO[,3]
for (i in 3:length(colnames(DO))){
DO[,i]<- as.numeric(as.character(DO[,i]))
}
str(DO)
DO[1:10,]
write.csv(DO,paste("/data/DO_",site,".csv",sep=""))
setwd('P:/Projects/GitHub_Prj/DO_Data')
write.csv(DO,paste("/data/DO_",site,".csv",sep=""))
write.csv(DO,paste("data/DO_",site,".csv",sep=""))
write.csv(DO,paste("data/DO_",site,".csv",sep=""),row.names=FALSE)
DO$depth<- DO$depth*0.3048
write.csv(DO,paste("data/DO_",site,".csv",sep=""),row.names=FALSE)
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01193463'
start_date  <-'2015-06-15';
end_date    <-'2015-09-22';
parameterCd <-'00060,00065,00010,00095,00300,00301';
###Combine into url for site#########
DO_parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(DO_parts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
DO[1:10,]
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01193500'
start_date  <-'2016-06-30';
end_date    <-'2016-09-22';
parameterCd <-'00060,00065,00010,00095,00300,00301';
###Combine into url for site#########
DO_parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(DO_parts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
DO[1:10,]
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01193500'
start_date  <-'2016-06-30';
end_date    <-'2016-09-22';
parameterCd <-'00010,00095,00300,00301';
FparameterCd<-'00060,00065';
###Combine into url for site#########
parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
flowparts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', FparameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(parts,sep='',collapse='');
Flow_url<-paste(flowparts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
Flow<-parse_fstat(readLines(Flow_url))
DO<-merge(DO,Flow,by=c("agency_cd","site_no","datetime","tz_cd"))
DO[1:10,]
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01193463'
flowsite    <- '01193500'
start_date  <-'2015-06-30';
end_date    <-'2015-09-22';
parameterCd <-'00010,00095,00300,00301';
FparameterCd<-'00060,00065';
###Combine into url for site#########
parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
flowparts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', FparameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(parts,sep='',collapse='');
Flow_url<-paste(flowparts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
Flow<-parse_fstat(readLines(Flow_url))
DO<-merge(DO,Flow,by=c("agency_cd","site_no","datetime","tz_cd"))
DO[1:10,]
Flow[1:5,]
Flow_url
flowparts <- c(base_url,'/iv/?format=rdb',
'&sites=',       flowsite,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', FparameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
Flow_url<-paste(flowparts,sep='',collapse='');
Flow<-parse_fstat(readLines(Flow_url))
Flow[1:10,]
DO<-merge(DO,Flow,by=c("agency_cd","site_no","datetime","tz_cd"))
DO[1:10,]
Flow[1:10,]
DO<-parse_fstat(readLines(DO_url))
DO<-merge(DO,Flow,by=c("agency_cd","datetime","tz_cd"))
DO[1:10,]
DO<-parse_fstat(readLines(DO_url))
Flow[1:10,]
DO[1:10,]
Flow<- Flow[,3:length(colnames(Flow))]
Flow[1:10,]
DO<-merge(DO,Flow,by=c("datetime","tz_cd"))
DO[1:10,]
base_url    <-'https://waterservices.usgs.gov/nwis';
site        <-'01195510'
flowsite    <-'01195490'
start_date  <-'2015-06-30';
end_date    <-'2015-09-22';
parameterCd <-'00010,00095,00300,00301';
FparameterCd<-'00060,00065';
###Combine into url for site#########
parts <- c(base_url,'/iv/?format=rdb',
'&sites=',       site,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', parameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
flowparts <- c(base_url,'/iv/?format=rdb',
'&sites=',       flowsite,
'&startDT=',     start_date,
'&endDT=',       end_date,
'&parameterCd=', FparameterCd,
'&siteType=',    'ST',
'&siteStatus=',   'all');
DO_url<-paste(parts,sep='',collapse='');
Flow_url<-paste(flowparts,sep='',collapse='');
DO<-parse_fstat(readLines(DO_url))
Flow<-parse_fstat(readLines(Flow_url))
Flow<- Flow[,3:length(colnames(Flow))]
DO[1:5,]
Flow[1:5,]
